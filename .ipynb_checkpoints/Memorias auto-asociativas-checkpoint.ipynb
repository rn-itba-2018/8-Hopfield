{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T17:54:41.641243Z",
     "start_time": "2018-05-03T17:54:41.628233Z"
    }
   },
   "source": [
    "# Redes de Hopfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorias\n",
    "    1. Asociativas\n",
    "        a. Hereoasociativa\n",
    "        b. Autoasociativa\n",
    "    2. Direccionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Memoria Asociativa:** \n",
    "- Forma de bÃºsqueda de recuperaciÃ³n de datos que se cree que tiene el cerebro\n",
    "- No utiliza direcciones para acceder al contenido.\n",
    "- Almacenamiento y recuperaciÃ³n de informaciÃ³n por asociaciÃ³n con otras informaciones.\n",
    "- Permite recuperar informaciÃ³n a partir de conocimiento parcial de su contenido.\n",
    "- Las memorias asociativas son una de las redes neuronales artificiales mÃ¡s importantes con un amplio rango de aplicaciones en Ã¡reas tales como: Memorias de acceso por contenido, identificaciÃ³n de patrones y control inteligente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Memorias heteroasociativas:** establecen una correspondencia de x (vector de entrada) en y (vector de salida), de distinta dimensiÃ³n. Dichos patrones se llaman memorias principales o de referencia.\n",
    "\n",
    "- **Memorias autoasociativas:** establece la misma correspondencia que la memoria heteroasociativa pero siendo los patrones de entrada y de salida los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria Heteroasociativa:\n",
    "\n",
    "Como dijimos anteriormente, podemos entrar con informaciÃ³n corrupta, o informaciÃ³n faltante:\n",
    "- Alto\n",
    "- Usa anteojos\n",
    "- Ingeniero en electrÃ³nica\n",
    "- Trabaja en el GEDA\n",
    "- Docente de TeorÃ­a de Circuitos\n",
    "- Morocho\n",
    "- Rulos\n",
    "- Pelo corto\n",
    "\n",
    "-> Daniel Jacoby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria Asociativa:\n",
    "Ejemplo de una memoria asociativa:\n",
    "![captcha](images/captcha.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes de Hopfield: ImplementaciÃ³n de una memoria Autoasociativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables de entrada (bits):** 1 y -1 (o 1 y 0)\n",
    "\n",
    "**Patrones de entrada:** palabras de N bits \n",
    "(Por ser una memoria autoasociativa la **salida** es una palabra de N bits tambiÃ©n)\n",
    "\n",
    "**Definiciones:**\n",
    "- **ğ‘**: ğ‘‡ğ‘ğ‘šğ‘Ã±ğ‘œ ğ‘’ğ‘› ğ‘ğ‘–ğ‘¡ğ‘  ğ‘‘ğ‘’ ğ‘™ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘ ğ‘¦ ğ‘™ğ‘ ğ‘ ğ‘ğ‘™ğ‘–ğ‘‘ğ‘\n",
    "- **ğœ‰**:ğ‘ƒğ‘ğ‘¡ğ‘ŸÃ³ğ‘› ğ‘‘ğ‘’ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘\n",
    "- **ğœ‰_ğ‘–**:ğ‘ğ‘–ğ‘¡ ğ‘– ğ‘‘ğ‘’ğ‘™ ğ‘ğ‘ğ‘¡ğ‘ŸÃ³ğ‘› ğ‘‘ğ‘’ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘ ğœ‰\n",
    "- **ğ‘†**:ğ‘ƒğ‘ğ‘¡ğ‘ŸÃ³ğ‘› ğ‘‘ğ‘’ ğ‘ ğ‘ğ‘™ğ‘–ğ‘‘ğ‘\n",
    "- **ğ‘†_ğ‘–**:ğ‘ğ‘–ğ‘¡ ğ‘– ğ‘‘ğ‘’ğ‘™ ğ‘ğ‘ğ‘¡ğ‘ŸÃ³ğ‘› ğ‘‘ğ‘’ ğ‘ ğ‘ğ‘™ğ‘–ğ‘‘ğ‘ ğ‘†\n",
    "- **ğœ**:ğ‘ƒğ‘ğ‘¡ğ‘Ÿğ‘œğ‘›ğ‘’ğ‘  ğ‘‘ğ‘’ ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘–ğ‘\n",
    "- **P**: Cantidad de patrones de referencia\n",
    "- **ğœ^ğœ‡**:ğ‘ƒğ‘ğ‘¡ğ‘ŸÃ³ğ‘› ğ‘‘ğ‘’ ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘–ğ‘ ğ‘›Ãºğ‘šğ‘’ğ‘Ÿğ‘œ ğœ‡\n",
    "- **ğœ_ğ‘–^ğœ‡**:ğ‘ğ‘–ğ‘¡ ğ‘– ğ‘‘ğ‘’ğ‘™ ğ‘ğ‘ğ‘¡ğ‘ŸÃ³ğ‘› ğ‘‘ğ‘’ ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘–ğ‘ã€– ğœã€—^ğœ‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La memoria se puede decir que guarda *Patrones de referencia* a partir de los cuales busca la soluciÃ³n.\n",
    "![captcha](images/memo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular la salida de la red de Hopfield como:\n",
    "\n",
    "$$ S_j = sign(\\sum_{i=1}^{N} w_{ij} ğœ‰_i) $$ \n",
    "\n",
    "![pinv](images/patroneinverso.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:05.430972Z",
     "start_time": "2018-05-08T19:53:04.534807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1 -1 -1 -1]\n",
      " [-1 -1 -1 -1  1  1  1]\n",
      " [-1 -1 -1 -1  1  1  1]\n",
      " [-1 -1 -1 -1  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import transpose as t\n",
    "\n",
    "P = np.array([[1, 1, 1, 1, -1, -1, -1]])\n",
    "W = P.T@P\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:05.442971Z",
     "start_time": "2018-05-08T19:53:05.430972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getReference(P):\n",
    "    return np.sign(W@P.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:05.510973Z",
     "start_time": "2018-05-08T19:53:05.502974Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMatrix(P):\n",
    "    return P.T@P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:06.233486Z",
     "start_time": "2018-05-08T19:53:06.205486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pc = np.array([[1, -1, 1, 1, -1, -1, -1]])\n",
    "(getReference(Pc) == P).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:06.757501Z",
     "start_time": "2018-05-08T19:53:06.745497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pc = np.array([[-1, -1, -1, 1, -1, 1, 1]])\n",
    "ref = getReference(Pc)\n",
    "(ref == P).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento de un patrÃ³n y su inverso\n",
    "En sistemas dinÃ¡micos un **atractor** es un conjunto de valores numÃ©ricos hacia los cuales un sistema tiende a evolucionar a partir de una amplia variedad de condiciones iniciales.\n",
    "\n",
    "Cuando calculamos la matriz W para que P sea un atractor, -P automÃ¡ticamente queda definido como atractor tambiÃ©n. \n",
    "\n",
    "**Cada vez que definimos a P como un patrÃ³n de referencia, -P tambiÃ©n lo es.**\n",
    "\n",
    "![captcha](images/atractores.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:08.006040Z",
     "start_time": "2018-05-08T19:53:07.994040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ref != P).all()  # Para cada elemento, ref[i] != P[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento de varios Patrones de Referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se desea almacenar varios patrones de referencia se suele usar:\n",
    "\n",
    "$ w_{i,j} = \\frac{1}{N} \\sum_{\\mu = 1}^P ğœ_i^\\mu ğœ_j^\\mu$\n",
    "\n",
    "Ã‰sta ecuaciÃ³n se la denomina *Regla de Hebb* y es el promedio de la matriz de pesos para cada patrÃ³n por separado.\n",
    "\n",
    "Observar:\n",
    "- Si para distintos patrones de referencia, la correlaciÃ³n entre los bits i y j se mantiene, $ğœ”_{ğ‘–,ğ‘—}$ tendrÃ¡ un valor en mÃ³dulo cercano a 1. \n",
    "- Si para distintos patrones de referencia, no hay correlaciÃ³n entre los bits i y j, $ğœ”_{ğ‘–,ğ‘—}$ tendrÃ¡ un valor en mÃ³dulo cercano a 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:09.555966Z",
     "start_time": "2018-05-08T19:53:09.515965Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defino los patrones de referencia\n",
    "P1=np.array([[ 1,  1,   1,   1,   1,   1,   1]]);\n",
    "P2=np.array([[ 1,  1,   1,  -1,  -1,  -1,  -1]]);\n",
    "P3=np.array([[-1, -1,   1,   1,   1,   1,   1]]);\n",
    "P4=np.array([[ 1,  1,   1,  -1,  -1,   1,   1]]);\n",
    " \n",
    "# Armo la matriz de pesos\n",
    "W1 = getMatrix(P1);\n",
    "W2 = getMatrix(P2);\n",
    "W3 = getMatrix(P3);\n",
    "W4 = getMatrix(P4);\n",
    " \n",
    "W = (W1 + W2 + W3 + W4) / 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:10.088000Z",
     "start_time": "2018-05-08T19:53:10.072004Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whichReference(P, ref_mat):\n",
    "    for i, p in enumerate(ref_mat):\n",
    "        if (getReference(P) == p).all():\n",
    "            sign = '-' if i > 3 else ''\n",
    "            num = str(i%4 + 1)\n",
    "            print(sign + 'P' + num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:10.891937Z",
     "start_time": "2018-05-08T19:53:10.859937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "P2\n",
      "-P4\n"
     ]
    }
   ],
   "source": [
    "# Calculo la salida para P3 \n",
    "print((getReference(P3) == P3).all())\n",
    " \n",
    "# Calculo la salida para -P3 \n",
    "print((getReference(-P3) == -P3).all())\n",
    " \n",
    "Ptot = [P1, P2, P3, P4, -P1, -P2, -P3, -P4]\n",
    "# Calculo la salida para P2 con error en el bit 4\n",
    "Pc = np.array([[1, 1, 1, 1, -1, -1, -1]])\n",
    "whichReference(Pc, Ptot)\n",
    "        \n",
    "# Calculo la salida para -P4 con un error en el bit 3\n",
    "Pc = np.array([[-1, -1, 1, 1, 1, -1, -1]])\n",
    "whichReference(Pc, Ptot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estabilidad de un patrÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CondiciÃ³n de estabilidad\n",
    "\n",
    "Normalmente, se espera que si se entrena con un patrÃ³n, el mismo deberÃ­a ser igual a la salida, ya que se entrenÃ³ justamente la red para que eso pase. Dicho de otra forma, se debe cumplir:\n",
    "\n",
    "$ sign(h_i^\\nu) = ğœ‰_i^\\nu $ para todo i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde $h_i^\\nu = \\sum_{j} w_{ij}^\\mu ğœ‰_j^\\nu$ que es lo que resulta de entrenar la red de hopfield con un patrÃ³n ($ğœ‰$).\n",
    "\n",
    "Desarrollando los pesos $w$ se llega a que:\n",
    "\n",
    "$$ h_i^\\nu = \\frac{1}{N} \\sum_{j}\\sum_{\\mu} ğœ‰_{i}^\\mu ğœ‰_{j}^{\\mu} ğœ‰_j^\\nu $$\n",
    "\n",
    "Sacamos finalmente factor comÃºn:\n",
    "\n",
    "\\begin{equation} \n",
    "h_i^\\nu = ğœ‰_i^\\nu + \\frac{1}{N} \\sum_{j}\\sum_{\\mu \\neq \\nu} ğœ‰_{i}^\\mu ğœ‰_{j}^{\\mu} ğœ‰_j^\\nu \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la Ãºltima ecuaciÃ³n se deduce que si el tÃ©rmino de la derecha, es nulo, se cumple la condiciÃ³n de estabilidad.\n",
    "De hecho, gracias a la funciÃ³n signo que se debe aplicar sobre $h_i^\\nu$, se puede ser menos restrictivo con el tÃ©rmino de la derecha y se puede decir que para que cumpla estabilidad debe darse la siguiente igualdad:\n",
    "\n",
    "\\begin{equation} \n",
    "    -ğœ‰_i^\\nu \\frac{1}{N} \\sum_{j}\\sum_{\\mu \\neq \\nu} ğœ‰_{i}^\\mu ğœ‰_{j}^{\\mu} ğœ‰_j^\\nu < 1 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacidad de una red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos entonces la siguiente magnitud:\n",
    "\n",
    "$$ C_i^\\nu = ğœ‰_i^\\nu \\frac{1}{N} \\sum_{j}\\sum_{\\mu \\neq \\nu} ğœ‰_{i}^\\mu ğœ‰_{j}^{\\mu} ğœ‰_j^\\nu $$\n",
    "\n",
    "De la condiciÃ³n de estabilidad vista anteriormente.\n",
    "Si $ C_i^\\nu > 1 $ entonces tendremos un bit erroneo en la salida. Es decir, si se ingresa a la red con un patrÃ³n de entrenamiento, a la salida se obtendrÃ¡ un patrÃ³n distinto al deseado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ C_i^\\nu $ depende Ãºnicamente de los patrones que queremos almacenar y de su cantidad. Si consideramos patrones de entrenamiento con igual ocurrencia de $  ğœ‰_{i}^\\mu = 1 $ y $  ğœ‰_{i}^\\mu = -1 $ entonces se puede calcular que la probablidad de que un bit sea inestable es:\n",
    "\n",
    "$$ P_{error} = Prob(C_i^\\nu > 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos ğ‘ƒ_ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ:\n",
    "- N es la cantidad de bits de la entrada/salida.\n",
    "- p es la cantidad de patrones almacenados.\n",
    "\n",
    "_Nota:_ Suponemos que N y p >> 1 (es una condiciÃ³n aceptable y simplificarÃ¡ la matemÃ¡tica)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ ğ¶_ğ‘–^ğœˆ $ es 1/N veces la suma de N*p (N*(p-1) para ser exactos) nÃºmeros aleatorios independientes que valen 1 o -1. DistribuciÃ³n de una suma de N*p valores que pueden valer 1 o -1 con p=0.5.\n",
    "\n",
    "Este problema puede ser modelado con una distribuciÃ³n binomial de media cero y varianza $ğœ^2=ğ‘/ğ‘$, pero por ser una suma de N tÃ©rminos, para $N*p>30$ puede ser modelado con una distribuciÃ³n gaussiana, valiÃ©ndonos del Teorema del LÃ­mite Central:\n",
    "\n",
    "![gauss](images/gaus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente tabla muestra las relaciones de p/N para obtener distintas probabilidades de error:\n",
    "![gauss](images/tablaerrores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede interpretar Ã©stos resultados siendo que cuanto mÃ¡s grandes sean los patrones (N mayor) mayor serÃ¡ la matrÃ­z de Hopfield y mayor su capacidad. Por el contrario, cuantos mÃ¡s patrones se deseen agregar (p), mÃ¡s aumentarÃ¡ su probabilidad de error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efecto Avalancha\n",
    "\n",
    "- Este calculo solo se refiere a la probabilidad de error cuando ingresamos con patrones sin errores. Ingresar a la red con patrones cuyos bits tengan errores, hace que el error sea aÃºn mayor.\n",
    "\n",
    "- En el peor caso puede ocurrir un efecto avalancha que haga que el nÃºmero de errores sea tan grande que el patrÃ³n a la salida no tenga nada que ver con el patrÃ³n almacenado.\n",
    "\n",
    "- Se puede demostrar que este efecto avalancha ocurre cuando p>0.138N.\n",
    "\n",
    "- Otra definiciÃ³n de capacidad es que la mayorÃ­a de los patrones puedan ser recuperados perfectamente (los Nbits son recuperados correctamente con una probabilidad del 99%). Esto equivale a pedir una probabilidad de error p<0.01*N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estados EspÃºreos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdemÃ¡s de almacenar un patrÃ³n inverso, hay otros patrones que funcionan como atractores. Dicho atractor que no es ni el patrÃ³n mismo ni el patrÃ³n inverso se lo denomina como estÃ¡do espÃºreo.\n",
    "Un ejemplo de patrÃ³n espÃºreo, surge de hacer la cominaciÃ³n lineal de un nÃºmero impar de patrones de entrenamiento, por ejemplo:\n",
    "\\begin{equation} \n",
    "ğœ‰_i^{mix} = sgn(\\pmğœ‰_i^{\\mu_1}\\pmğœ‰_i^{\\mu_2}\\pmğœ‰_i^{\\mu_3})\n",
    "\\end{equation}\n",
    "\n",
    "Para un valor de $ğœ‰_i^{\\mu_1}$ dado $ğœ‰_i^{mix}$ tendrÃ¡ el mismo signo 3 de las 4 combinaciones posibles de $ğœ‰_i^{\\mu_2}$ y $ğœ‰_i^{\\mu_3}$ por lo tanto $ğœ‰_i^{mix}$ estarÃ¡ a una distancia de Hamming N/4 de $ğœ‰_i^{\\mu_1}$. un razonamiento anÃ¡logo nos lleva a que la distancia de Hamming de $ğœ‰_i^{mix}$ con respecto a $ğœ‰_i^{\\mu_2}$ y $ğœ‰_i^{\\mu_3}$ tambiÃ©n es N/4.\n",
    "\n",
    "Esto implica que el producto escalar entre $ğœ‰_i^{mix}$ y $ğœ‰_i^{\\mu_1}$ valga N/2.\n",
    "\n",
    "Por lo tanto, nos queda:\n",
    "\\begin{equation} \n",
    "h_i^{mix} =  \\frac{1}{N} \\sum_{j\\mu} ğœ‰_{i}^\\mu ğœ‰_{j}^{\\mu} ğœ‰_j^{mix} = \\frac{1}{2}ğœ‰_i^{\\mu_2}+\\frac{1}{2}ğœ‰_i^{\\mu_3}+crossterms\n",
    "\\end{equation}\n",
    "\n",
    "Los cross-terms tienen la particularidad estadÃ­stica (distribuaciÃ³n binamial) analizada anteriormente, por lo que la ecuaciÃ³n anterior cumple con el criterio de estabilidad. Por lo tanto $ğœ‰_i^{mix}$ es un patrÃ³n almacenado en la red.\n",
    "\n",
    "AdemÃ¡s Amit (1985) demostrÃ³ que existen otros estados espÃºreos en la red que no guardan correlaciÃ³n con los estados ya almacenados.\n",
    "\n",
    "Por lo tanto, la red de Hopfield estÃ¡ limitada no solo por la probabilidad del error de bit, sino que admÃ¡s por la existencia de estados espÃºreos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MecÃ¡nica estadÃ­stica de sistemas magnÃ©ticos\n",
    "\n",
    " - Hay una relaciÃ³n estrecha entre las redese de Hopfield y algunos modelos magnÃ©ticos sencillos de la fÃ­sica estadÃ­stica.\n",
    " - La analogÃ­a es muy Ãºtil cuando se utilizan unidades estocÃ¡sticas, de donde se establece el concepto de temperatura de la red.\n",
    " - Repasaremos algunos conceptos bÃ¡sicos de materiales magnÃ©ticos.\n",
    "   - Se pueden representar como un conjunto de dipolos magnÃ©ticos de tamaÃ±o atÃ³mico, dispuestos de acuerdo a una estructura cristalina. A cada dipolo lo llamaremos SPIN.\n",
    "   - Cada spin podrÃ¡ estar alineado en dos direcciones posibles.\n",
    "   - cada spin serÃ¡ identificado por una variable $S_i$. Si el spin i estÃ¡ orientado hacia arriba, entonces $S_i=1$. Si el spin i estÃ¡ orientado hacia abajo $S_i=-1$\n",
    "   \n",
    "   \n",
    " - La representaciÃ³n grÃ¡fica de un material magnÃ©tico nos queda de la siguiente forma:\n",
    " ![field](images/field.png)\n",
    " - Para la red neuronal anÃ¡loga, una neurona activada equivale a un spin +1 y una neurona desactivada un spin -1.\n",
    " - El comportamiento de cada spin queda definido por el campo magnÃ©tico existente en su ubicaciÃ³n, el cual estÃ¡ compuesto por el campo generado por los otros spins (campo interno), mas la contribuciÃ³n de un posible campo magnÃ©tico externo.\n",
    " - Dichas contribuciones sobre el spin i pueden ser expresadas como:\n",
    "\\begin{equation} \n",
    "h_i =   \\sum_{j} w_{ij} S_{j}+ h^{ext}\n",
    "\\end{equation}\n",
    " - Donde cada coeficiente $w_{ij}$ miden la influencia de cada spin j sobre el spin i. En un material magnÃ©tico estos coeficientes son simÃ©tricos.\n",
    " - El campo magnÃ©tico $h_i$ a bajas temperaturas define la dinÃ¡mica del spin i ya que el spin tiende a alinearse segÃºn la direcciÃ³n de $h_i$.\n",
    "   - Por lo tanto el valor de $S_i = sgn(h_i)$\n",
    "   - Esta actualizaciÃ³n se hace en forma asincrÃ³nica.\n",
    " - Otra forma de definir la interacciÃ³n de los spins es a travÃ©s de la energÃ­a potencial vinculada a esa interacciÃ³n. Para un material magnÃ©tico cuyos spins estÃ¡n influenciados segÃºn $\\omega_{ij}$, su energÃ­a potencial es:\n",
    "\\begin{equation} \n",
    "H =  -\\frac{1}{2} \\sum_{ij} w_{ij} S_{i}S_{j}- h^{ext}\\sum_{i}S_i\n",
    "\\end{equation}\n",
    " - Por lo tanto la analogÃ­a con una red de Hopfield es completa\n",
    "   - Los coeficientes de interacciÃ³n entre los spins son los pesos\n",
    "   - La entrada a una neurona corresponde al estado de cada uno de los spins.\n",
    "   - La suma resultante de la interacciÃ³n de todos los spins j en un spin i mas el campo externo, corresponde a la salida del combinador lineal.\n",
    " - A este modelo de un material ferromagnÃ©tico se lo denomina ** Modelo de Issing **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efecto de la temperatura en el modelo de Issing\n",
    " - El modelo anterior aplica solamente a temperaturas muy bajas, cercanas al 0 K.\n",
    " - Para temeraturas mÃ¡s altas, el comportamiento de los spins se vuelve estocÃ¡stico y su estado de alineaciÃ³n esta determinado por: $S_i = +1$ on probabilidad $g(h_i)$ y $S_i$ con probabilidad $1-g(h_i)$.\n",
    " - La funciÃ³n $g(h_i)$ depende de la temperatura del sistema:\n",
    "\\begin{equation} \n",
    "g(h) = f_\\beta(h)\\equiv \\frac{1}{1+exp(-2\\beta h)}\n",
    "\\end{equation}\n",
    " - Ya que $\\beta=\\frac{1}{\\kappa_B T}$, donde $\\kappa_B$ es la constante de Boltzmann y vale $1.38*10^{16\\frac{erg}{K}}$.\n",
    " - Dado que: $1-f_\\beta(h)=f_\\beta(-h)$, podemos escribir: $Prob(S_i = \\pm1)=f_\\beta(\\pm h_i)=\\frac{1}{1+exp(\\mp2\\beta h_i)}$\n",
    " - La temperatura regula quÃ© tan abrupta es la variaciÃ³n de la pdf alrededor de h=0.\n",
    " - A medida que $T\\rightarrow0$, vemos como el modelo estocÃ¡stico se reduce al modelo determinÃ­stico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado de equilibrio de un spin\n",
    " - Vamos a analizar cual es la magnetizaciÃ³n promedio de un material ferromagnÃ©tico.\n",
    " - Para empezar, supongamos que tenemos un material ferromagnÃ©tico de un solo spin.\n",
    "   - Esto implicarÃ¡ que el Ãºnico campo magnÃ©tico serÃ¡ el externo.\n",
    "   - Su valor promedio tendrÃ¡ la forma:\n",
    "\\begin{equation}\n",
    "\t\\langle S \\rangle = Prob(+1).(+1)+ Prob(-1).(-1) \\\\\n",
    "    = \\frac{1}{1+e^{-2\\beta h}}-\\frac{1}{1+e^{2\\beta h}}=\\frac{e^{\\beta h}}{e^{\\beta h}+e^{-\\beta h}}-\\frac{e^{-\\beta h}}{e^{-\\beta h}+e^{\\beta h}} \\\\\n",
    "    =tangh(\\beta h)\n",
    "\\end{equation}\n",
    " - La tanh tiene la misma forma que la distribuciÃ³n de probabilidades, pero varia entre -1 y 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TerorÃ­a del campo medio\n",
    " - Ahora queremos analizar que pasa en un material ferromagnÃ©tico con mÃ¡s de un spin.\n",
    " - La evoluciÃ³n del estado de un spin i depende de:\n",
    "\\begin{equation}\n",
    " h_i=\\sum_i w_{ij}S_j+h^{ext}\n",
    "\\end{equation}\n",
    "   - Lo cual involucra variables S_j que fluctuÃ¡n en el tiempo.\n",
    "  - Como el anÃ¡lisis de un material ferromagnÃ©tico es muy complejo, se utiliza la teorÃ­a del campo medio para analizar el comportamiento de material y su magnetizaciÃ³n promedio.\n",
    "  - MÃ¡s allÃ¡ del interÃ©s que pueda tener fÃ­sicamente este anÃ¡lisis, mÃ¡s adelante noss brindarÃ¡ una herramienta mÃ¡s para analizar redes neuoronales.\n",
    "  - La idea consiste en reemplazar a cada uno de los $h_i$ resultantes en  cada spin, por su valor medio:\n",
    "  \\begin{equation}\n",
    "\t\\langle h_i \\rangle =\\sum_j w_{ij}\\langle S_j \\rangle + h^{ext}\n",
    "   \\end{equation}\n",
    "  - El mismo depende de los valores promedios de los spins del material.\n",
    "  - A partir de lo visto en \"Esta de equilibrio de un spin\" podemos calcular los valores de campo magnÃ©tico de cada spin del material como:\n",
    "  \\begin{equation}\n",
    "\t\\langle S_i \\rangle = tanh(\\beta \\langle h_i \\rangle) = tanh(\\beta\\sum_j w_{ij}\\langle S_j \\rangle +\\beta h^{ext})\n",
    "  \\end{equation}\n",
    "    - Lo cual nos da N ecuaciones no lineales con N incÃ³gnitas, pero sin que en ningÃºn lado aparezcan variables aleatorias.\n",
    "  - La idea es reemplazar todos los sins salvo por sus valores medios, de esta forma podemos analizar como evoluciona un spin en particular.\n",
    "  - A medida que la cantidad de spins aumenta, este modelo se vuelve cada vez mÃ¡s preciso.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material ferromagnÃ©tico\n",
    " - Todos los $w_{ij}$ son positivos. Esto hace que todos los spins tiendan a alinearse los unos a los otros.\n",
    " - A partir de cierta temperatura $T_C$, sin campo externo,$\\langle S \\rangle =0$. Por debajo de esa temperatura $T_C,\\langle S \\rangle \\neq 0$\n",
    "   - Por ejemplo el hierro pierde sus propiedades magnÃ©ticas a 770 grados.\n",
    " - El modelo mÃ¡s sencillo de material ferromagnÃ©tico estÃ¡ dado por $w_{ij}=\\frac{J}{N}$\n",
    "   - Para un ferromagnÃ©tico de N spins.\n",
    "   - Este modelo tiene su anÃ¡logo en la red de Hopfield. Si J=1, corresponde a un solo patrÃ³n almacenado, cuyos bits valen todos 1.\n",
    "   - Es decir. hay dos atractores, uno con todos los valores en 1 y otro con todos los valores en -1. Dicho de otra forma, todos los spins alineados hacia arriaba y todos los spins alineados hacia abajo.\n",
    "   - Si hablamos de una red con varios patrones almacenados, el fenÃ³meno es como si hablÃ¡ramos de un material ferromagnÃ©tico cuyos valores de spins estables ya no son todos 1 o todos -1, sino que aparecen otras opciones.\n",
    " - En temperatura cero si la mayorÃ­a de los spins apuntan en una direcciÃ³n, podemos dejar que el sistema evolucione en el tiempo hasta que todos queden apuntando en esa direcciÃ³n.\n",
    " - Para una temperatura dada, podemos analizar que pasa con el material ferromagnÃ©tico usando la teorÃ­a de campo medio. En el caso ferromagnÃ©tico con $w_{ij}=\\frac{J}{N}$ nos queda:\n",
    "   \\begin{equation}\n",
    "\t\\langle S \\rangle = tanh(\\beta J\\langle S \\rangle) \n",
    "  \\end{equation}\n",
    "    - Para todos los spins sin la presencia de campo magnÃ©tico externo.\n",
    "    - Esta ecuaciÃ³n puede ser resuelta grÃ¡ficamente:\n",
    "    ![ferrofun](images/ferrofun.png)\n",
    " - La soluciÃ³ varia segÃºn $\\beta J$ sea mayor o menor a 1.\n",
    " - Esto hace que haya una temperatura crÃ­tica a partir de la cual la media del valor spin se hace cero \n",
    " \\begin{equation}\n",
    "\tT_C=\\frac{J}{\\kappa_B}\n",
    "  \\end{equation}\n",
    "  - El grÃ¡fico del valor medio del spin en funciÃ³n de la temperatura (para una de las soluciones) quedarÃ­a:\n",
    "  ![temp](images/temp.png)\n",
    "  - Los spins pueden encontrarse predominantemente hacia arriba o hacia abajo cuando $T<T_C$.\n",
    "  - Si $N\\rightarrow\\infty$, entonces el sistema va a permanecer en el estadoe n el que se encuentre.\n",
    "  - Siendo que en general N es lo suficientemente grande, la teorÃ­a del campo medio describe correctamente el comportamiento de un material ferromagnÃ©tico en funciÃ³n de la temperatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes estocÃ¡sticas\n",
    " - Haremos que el comportamiento de las neuronas de una red de Hopfield sea idÃ©ntico al de los spins en el modelo de Issing.\n",
    " - Es decir, reemplazaremos la funciÃ³n de activaciÃ³n por una funciÃ³n de activaciÃ³n estocÃ¡stica que cumpla con:\n",
    " \\begin{equation}\n",
    " Prob(S_i = \\pm1)=f_\\beta(\\pm h_i)=\\frac{1}{1+exp(\\mp2\\beta h_i)}\n",
    " \\end{equation}\n",
    "   - Para la neurona i seleccionada en forma aleatoria para ser actualizara.\n",
    " - Se puede definir uns pseudo-temperatura de la red como: $\\beta \\equiv \\frac{1}{T}$\n",
    " - Dicha temperatura modifica la pendiente de la sigmoidea alrededor de h=0.\n",
    "   - A baja temperatura la sigmoidea se transforma en una funciÃ³n escalÃ³n y nos queda una red determinÃ­stica.\n",
    "   - A medida que sube la temperatura la pendiente es cada vez menos pronunciada.\n",
    " - El uso de unidades estocÃ¡sticas no solos nos permite modelar el comportamiento de una red neuronal real, sino que es Ãºtil en muchas situaciones ya que veremsos que nos permite eliminar varios mÃ­nimos locales de la funciÃ³n de enrgÃ­a pertenecientes a estados espurios.\n",
    "   - En general los mÃ­nimos correspondientes a estados espurios tienen mayor energÃ­a (menor estabilidad) que los patrones almacenados.\n",
    " - En base a esto, debemos analizar los los estaos de equilibrio de una red nauronal, esto es, aplicar el concepto de teorÃ­a de campo medio para ver quÃ© pasa con cada una de las neuronas a medida que el sistema evoluciona.\n",
    "   - Se puede probar que una red de pesos simÃ©tricos con funciÃ³n de energÃ­a asociada, tiene de a converger a un estado de equilibrio.\n",
    " - Si bien no podemos hablar de configuraciones estables en funciÃ³n de valores determinÃ­siticos de $S_i$, si podemos ver en que valores medidos de $S_i$ se estabiliza cada una de las neuronas.\n",
    " ### Redes estocÃ¡sticas (TeorÃ­a de campo medio)\n",
    " - Nos mantendremos en el casop de $p<<N$ o bien $N\\rightarrow \\infty$.\n",
    " - Podemos escribir las ecuaciones de campo medio como:\n",
    "   \\begin{equation}\n",
    "\t\\langle S_i \\rangle = tanh(\\frac{\\beta}{N}\\sum_{i,\\mu}ğœ‰_{i}^{\\mu}ğœ‰_{j}^{\\mu} \\langle S_i \\rangle)\n",
    "  \\end{equation}\n",
    "   - La soluciÃ³n es muy difÃ­cil de obtener ya que tenemos un sistema de N ecuaciones no lineales con N incÃ³gnitas.\n",
    " - Siguiendo el ejemplo del material ferromagnÃ©tico podemos suponer que:\n",
    " \\begin{equation}\n",
    "\t\\langle S_i \\rangle =mğœ‰_{i}^{\\nu}\n",
    "  \\end{equation}\n",
    "   - Es una soluciÃ³n del sistema de ecuaciones.\n",
    "   - Es decir, una de las soluciones es proporcional a uno de los patrones almacenados.\n",
    " - Ya vimos que en el caso de la red determinÃ­stica esto es cierto y con m=1.\n",
    " - Siguiendo la suposiciÃ³n, podemos escribir:\n",
    "    \\begin{equation}\n",
    "\tmğœ‰_{i}^{\\nu} = tanh(\\frac{\\beta}{N}\\sum_{i,\\mu}ğœ‰_{i}^{\\mu}ğœ‰_{j}^{\\mu}mğœ‰_j^{\\nu})\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como en el de la red estocÃ¡stica podemos escribir la sumatoria como un tÃ©rmino proporcional a $ğœ‰_{i}^{\\nu}$ mas un crosstalk term tiende a cero y nos queda:\n",
    "   \\begin{equation}\n",
    "\tmğœ‰_{i}^{\\nu} = tanh(\\beta m ğœ‰_j^{\\nu})\n",
    "  \\end{equation}\n",
    "- Pero como tanh es una funciÃ³n impar, podemos escribir independientemente del valor de $ğœ‰_j^{\\nu}$ : \n",
    "   \\begin{equation}\n",
    "\tm = tanh(\\beta m )\n",
    "  \\end{equation}\n",
    "- Esto es muy similar a lo que encontramos en el caso de la magnetizaciÃ³n de un ferromagnÃ©tico, por lo que podemos resolver de la misma manera (en forma grÃ¡fica); por lo tanto la temperatura crÃ­tica es igual a $T_C$ es igual a 1 para una red estocÃ¡stica con $p<<N$.\n",
    "- Adaptando el problema de Issing a una red neuronal, podemos escribir:\n",
    "   \\begin{equation}\n",
    "   m = \\frac{\\langle S_i \\rangle}{ğœ‰_j^{\\nu}}=\\text{Prob(bit is correct)} - \\text{Prob(bit is incorrect)}\n",
    "  \\end{equation}\n",
    "- Y por lo tanto el nÃºmero promedio de bits correctos es:\n",
    "   \\begin{equation}\n",
    "   \\langle N_correct \\rangle = \\frac{1}{2}N(1+m)\n",
    "  \\end{equation}\n",
    "- Lo podemos graficar en funciÃ³n de de la temperatura:\n",
    "![temp2](images/temp2.png)\n",
    "- Si bien se eliminan estados espurios de mayor energÃ­a, otros estados como los inversos permanecen en la red.\n",
    "- Los estados mixtos siguen presente, pero cada uno con una temperatura crÃ­tica distinta a partir de la cual deja de ser estable.\n",
    "- La temperatura mÃ¡s alta a partir de la cual los estados mixtos dejan de ser estables es la correspondiente a 3 estados mezclados. Dicha temperatura vale T=0.46.\n",
    "- Esto muestra como el ruido (a travÃ©s del concepto de temperatura) mejora la performance de la red.\n",
    "- La siguiente figura muestra esquemÃ¡ticamente cÃ³mo varÃ­a la funciÃ³n de energÃ­a al aumentar la temperatura de la red:\n",
    "![espures](images/espureos.png)\n",
    "- NÃ³tese que el anÃ¡lisis de teorÃ­a de campo medio es independiente del modo de actualizaciÃ³n (ya sea sincrÃ³nica o asincrÃ³nica) ya que se independeiza de la dinÃ¡mica temporal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Weak Dilution\n",
    "\n",
    "Vamos a simular el proceso de muerte celular removiendo algunos pesos de la red aleatoriamente. Es decir:\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "f(x) = \\left\\lbrace\n",
    "\\begin{array}{ll}\n",
    "Hebb Value & c\\\\\n",
    "0 & 1 - c\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Donde c es una probabilidad que representa la proporciÃ³n de los pesos que mantendremos.\n",
    "\n",
    "Definamos $ğ¶_{ğ‘–ğ‘—}=1$ si deseamos mantener el peso ij y $ğ¶_{ğ‘–ğ‘—}=0$ en caso contrario. Podemos definir:\n",
    "\n",
    "$$ w_{ij} = C_{ij} w_{ij}^{Hebb} $$\n",
    "\n",
    "Quedando el valor a la entrada de la funciÃ³n de activaciÃ³n:\n",
    "\n",
    "$$ h_{ij} = \\sum_{j} C_{ij} w_{ij}^{Hebb} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como solo es removida una fracciÃ³n de las conexiones, a medida que Nïƒ âˆ, la cantidad de conexiones existentes seguirÃ¡ siendo infinita. Por lo tanto podemos escribir:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\langle h_i \\rangle = c \\sum_{j} w_{ij}^{Hebb} \\langle S_i \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "Esto hace que varÃ­en los valores medios a la entrada de los elementos estocÃ¡sticos haciendo que la temperatura efectiva de la red sea la temperatura original multiplicada por 1/c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong Dilution\n",
    "\n",
    "Analicemos quÃ© pasa cuando un nÃºmero infinitesimal de conexiones permanecen correctamente.\n",
    "\n",
    "Llamaremos K al nÃºmero promedio de conexiones que permanecen desde y hasta cada una de las unidades.\n",
    "$K < log N$ cuando $N \\to âˆ$.\n",
    "\n",
    "La diluciÃ³n se hace independientemente para $ğ‘¤_{ğ‘–ğ‘—}$ y $ğ‘¤_{ğ‘—ğ‘–}$, por lo que $ğ¶_{ğ‘–ğ‘—}$ y $ğ¶_{ğ‘—ğ‘–}$ son variables aleatorias independientes. La matrix W ya no es simÃ©trica.\n",
    "\n",
    "Definiremos los pesos como:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw_{ij} = \\frac{ğ¶_{ğ‘–ğ‘—}}{K} \\sum_{\\mu} ğœ‰_{i}^{\\mu}ğœ‰_{j}^{\\mu}\n",
    "\\end{equation}\n",
    "\n",
    "El 1/N fue reemplazado por 1/K para tener todos los valores cercanos a la unidad.\n",
    "Podemos expresar $â„_ğ‘–$ como:\n",
    "\n",
    "\\begin{equation}\n",
    "\th_{i} = \\sum_{j} w_{ij} S_j = \\frac{1}{K} \\sum_{j} ğ¶_{ğ‘–ğ‘—}  \\sum_{\\mu} ğœ‰_{i}^{\\mu}ğœ‰_{j}^{\\mu} S_j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un patrÃ³n almacenado ğœ‰_ğ‘–^ğœˆ podemos desglosar â„_ğ‘– en funciÃ³n de la salida esperada y el tÃ©rmino de crosstalk:\n",
    "\n",
    "\\begin{equation}\n",
    "\th_{i} = \\sum_{j} w_{ij} S_j = \\frac{1}{K} ğœ‰_{i}^{\\nu} \\sum_{j} ğ¶_{ğ‘–ğ‘—} ğœ‰_{j}^{\\nu} S_j + \\eta_i^\\nu\n",
    "\\end{equation}\n",
    "\n",
    "Ddonde\n",
    "$$ \\eta_i^\\nu = \\frac{1}{K} \\sum_{\\mu \\neq \\nu } ğœ‰_{i}^{\\mu} \\sum_{j} ğ¶_{ğ‘–ğ‘—} ğœ‰_{j}^{\\nu} S_j $$\n",
    "\n",
    "En este caso el crosstalk term depende del estado actual $ğ‘†_ğ‘—$.\n",
    "Si hacemos $ğ‘†_ğ‘–=ğœ‰_ğ‘–^ğœˆ$, entonces el tÃ©rmino de la derecha da en promedio $ ğœ‰_ğ‘–^ğœˆ$ ya que $âŸ¨âˆ‘_ğ‘— ğ¶_{ğ‘–ğ‘—} âŸ©=ğ¾$. El tÃ©rmino de crosstalk se reduce a 1/ğ¾ veces la suma de aproximadamente K*p variables aleatorias independientes (cuyo valor varÃ­a entre 1 y -1).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como $ğ‘†_ğ‘– = ğ‘ ğ‘”ğ‘›(â„_ğ‘–)$ podemos escribir:\n",
    "\n",
    "$$ m_\\nu = \\frac{1}{N} \\sum_i  sign( ğœ‰_{i}^{\\nu} h_i) = \\frac{1}{N} \\sum_i  sign(m_\\nu + ğœ‰_{i}^{\\nu} \\eta_i^\\nu) $$\n",
    "\n",
    "Para pasar del segundo tÃ©rmino al tercero, utilizamos:\n",
    "\n",
    "$$ h_i = \\sum_j w_{ij} S_j = \\frac{1}{K} ğœ‰_{i}^{\\nu} \\sum_{j} ğ¶_{ğ‘–ğ‘—} ğœ‰_{j}^{\\nu} S_j + \\eta_i^\\nu $$\n",
    "\n",
    "Para saber cuÃ¡nto se parece el estado actual a un patrÃ³n estable podemos ver cuÃ¡nto vale la media de $ğ‘š_ğœˆ$. La misma nos queda:\n",
    "\n",
    "$$ h_i = \\int{dn \\cdot P(\\eta) \\cdot sign(m_\\nu + \\eta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede demostrar que si $K<<M$ entonces queda:\n",
    "\n",
    "$$ m_\\nu = erf(\\frac{m_\\nu}{\\sqrt{2\\alpha '}}) $$\n",
    "\n",
    "En esencia es un problema muy similar al de x=tanh(x), ya que la funciÃ³n tanh y erf son muy similares:\n",
    "\n",
    "![strong](images/strongdilution.png)\n",
    "\n",
    "La pendiente en el origen en vez de valer 1 vale $\\frac{2}{\\sqrt{ğœ‹}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede resolver grÃ¡ficamente y obtenemos una soluciÃ³n de la misma forma que $<S>$ en funciÃ³n de la temperatura, pero en este caso $ğ›¼_ğ‘^â€²$ actÃºa de temperatura. \n",
    "\n",
    "Hay un valor crÃ­tico de $ğ›¼_ğ‘^â€²=2/ğœ‹$ a partir del cual la Ãºnica soluciÃ³n es $ğ‘š_ğ‘£=0$ , lo cual implica que se perdiÃ³ la capacidad de la red de almacenar patrones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "118px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

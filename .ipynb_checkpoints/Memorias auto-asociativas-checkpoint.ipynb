{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T17:54:41.641243Z",
     "start_time": "2018-05-03T17:54:41.628233Z"
    }
   },
   "source": [
    "# Redes de Hopfield"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memorias\n",
    "    1. Asociativas\n",
    "        a. Hereoasociativa\n",
    "        b. Autoasociativa\n",
    "    2. Direccionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Memoria Asociativa:** \n",
    "- Forma de búsqueda de recuperación de datos que se cree que tiene el cerebro\n",
    "- No utiliza direcciones para acceder al contenido.\n",
    "- Almacenamiento y recuperación de información por asociación con otras informaciones.\n",
    "- Permite recuperar información a partir de conocimiento parcial de su contenido.\n",
    "- Las memorias asociativas son una de las redes neuronales artificiales más importantes con un amplio rango de aplicaciones en áreas tales como: Memorias de acceso por contenido, identificación de patrones y control inteligente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Memorias heteroasociativas:** establecen una correspondencia de x (vector de entrada) en y (vector de salida), de distinta dimensión. Dichos patrones se llaman memorias principales o de referencia.\n",
    "\n",
    "- **Memorias autoasociativas:** establece la misma correspondencia que la memoria heteroasociativa pero siendo los patrones de entrada y de salida los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria Heteroasociativa:\n",
    "\n",
    "Como dijimos anteriormente, podemos entrar con información corrupta, o información faltante:\n",
    "- Alto\n",
    "- Usa anteojos\n",
    "- Ingeniero en electrónica\n",
    "- Trabaja en el GEDA\n",
    "- Docente de Teoría de Circuitos\n",
    "- Morocho\n",
    "- Rulos\n",
    "- Pelo corto\n",
    "\n",
    "-> Daniel Jacoby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memoria Asociativa:\n",
    "Ejemplo de una memoria asociativa:\n",
    "![captcha](images/captcha.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes de Hopfield: Implementación de una memoria Autoasociativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables de entrada (bits):** 1 y -1 (o 1 y 0)\n",
    "\n",
    "**Patrones de entrada:** palabras de N bits \n",
    "(Por ser una memoria autoasociativa la **salida** es una palabra de N bits también)\n",
    "\n",
    "**Definiciones:**\n",
    "- **𝑁**: 𝑇𝑎𝑚𝑎ñ𝑜 𝑒𝑛 𝑏𝑖𝑡𝑠 𝑑𝑒 𝑙𝑎 𝑒𝑛𝑡𝑟𝑎𝑑𝑎 𝑦 𝑙𝑎 𝑠𝑎𝑙𝑖𝑑𝑎\n",
    "- **𝜉**:𝑃𝑎𝑡𝑟ó𝑛 𝑑𝑒 𝑒𝑛𝑡𝑟𝑎𝑑𝑎\n",
    "- **𝜉_𝑖**:𝑏𝑖𝑡 𝑖 𝑑𝑒𝑙 𝑝𝑎𝑡𝑟ó𝑛 𝑑𝑒 𝑒𝑛𝑡𝑟𝑎𝑑𝑎 𝜉\n",
    "- **𝑆**:𝑃𝑎𝑡𝑟ó𝑛 𝑑𝑒 𝑠𝑎𝑙𝑖𝑑𝑎\n",
    "- **𝑆_𝑖**:𝑏𝑖𝑡 𝑖 𝑑𝑒𝑙 𝑝𝑎𝑡𝑟ó𝑛 𝑑𝑒 𝑠𝑎𝑙𝑖𝑑𝑎 𝑆\n",
    "- **𝜍**:𝑃𝑎𝑡𝑟𝑜𝑛𝑒𝑠 𝑑𝑒 𝑟𝑒𝑓𝑒𝑟𝑒𝑛𝑐𝑖𝑎\n",
    "- **P**: Cantidad de patrones de referencia\n",
    "- **𝜍^𝜇**:𝑃𝑎𝑡𝑟ó𝑛 𝑑𝑒 𝑟𝑒𝑓𝑒𝑟𝑒𝑛𝑐𝑖𝑎 𝑛ú𝑚𝑒𝑟𝑜 𝜇\n",
    "- **𝜍_𝑖^𝜇**:𝑏𝑖𝑡 𝑖 𝑑𝑒𝑙 𝑝𝑎𝑡𝑟ó𝑛 𝑑𝑒 𝑟𝑒𝑓𝑒𝑟𝑒𝑛𝑐𝑖𝑎〖 𝜍〗^𝜇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La memoria se puede decir que guarda *Patrones de referencia* a partir de los cuales busca la solución.\n",
    "![captcha](images/memo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos calcular la salida de la red de Hopfield como:\n",
    "\n",
    "$$ S_j = sign(\\sum_{i=1}^{N} w_{ij} 𝜉_i) $$ \n",
    "\n",
    "![pinv](images/patroneinverso.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:05.430972Z",
     "start_time": "2018-05-08T19:53:04.534807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1 -1 -1 -1]\n",
      " [ 1  1  1  1 -1 -1 -1]\n",
      " [-1 -1 -1 -1  1  1  1]\n",
      " [-1 -1 -1 -1  1  1  1]\n",
      " [-1 -1 -1 -1  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import transpose as t\n",
    "\n",
    "P = np.array([[1, 1, 1, 1, -1, -1, -1]])\n",
    "W = P.T@P\n",
    "\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:05.442971Z",
     "start_time": "2018-05-08T19:53:05.430972Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getReference(P):\n",
    "    return np.sign(W@P.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:05.510973Z",
     "start_time": "2018-05-08T19:53:05.502974Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMatrix(P):\n",
    "    return P.T@P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:06.233486Z",
     "start_time": "2018-05-08T19:53:06.205486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pc = np.array([[1, -1, 1, 1, -1, -1, -1]])\n",
    "(getReference(Pc) == P).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:06.757501Z",
     "start_time": "2018-05-08T19:53:06.745497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pc = np.array([[-1, -1, -1, 1, -1, 1, 1]])\n",
    "ref = getReference(Pc)\n",
    "(ref == P).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento de un patrón y su inverso\n",
    "En sistemas dinámicos un **atractor** es un conjunto de valores numéricos hacia los cuales un sistema tiende a evolucionar a partir de una amplia variedad de condiciones iniciales.\n",
    "\n",
    "Cuando calculamos la matriz W para que P sea un atractor, -P automáticamente queda definido como atractor también. \n",
    "\n",
    "**Cada vez que definimos a P como un patrón de referencia, -P también lo es.**\n",
    "\n",
    "![captcha](images/atractores.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:08.006040Z",
     "start_time": "2018-05-08T19:53:07.994040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ref != P).all()  # Para cada elemento, ref[i] != P[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Almacenamiento de varios Patrones de Referencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se desea almacenar varios patrones de referencia se suele usar:\n",
    "\n",
    "$ w_{i,j} = \\frac{1}{N} \\sum_{\\mu = 1}^P 𝜍_i^\\mu 𝜍_j^\\mu$\n",
    "\n",
    "Ésta ecuación se la denomina *Regla de Hebb* y es el promedio de la matriz de pesos para cada patrón por separado.\n",
    "\n",
    "Observar:\n",
    "- Si para distintos patrones de referencia, la correlación entre los bits i y j se mantiene, $𝜔_{𝑖,𝑗}$ tendrá un valor en módulo cercano a 1. \n",
    "- Si para distintos patrones de referencia, no hay correlación entre los bits i y j, $𝜔_{𝑖,𝑗}$ tendrá un valor en módulo cercano a 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:09.555966Z",
     "start_time": "2018-05-08T19:53:09.515965Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defino los patrones de referencia\n",
    "P1=np.array([[ 1,  1,   1,   1,   1,   1,   1]]);\n",
    "P2=np.array([[ 1,  1,   1,  -1,  -1,  -1,  -1]]);\n",
    "P3=np.array([[-1, -1,   1,   1,   1,   1,   1]]);\n",
    "P4=np.array([[ 1,  1,   1,  -1,  -1,   1,   1]]);\n",
    " \n",
    "# Armo la matriz de pesos\n",
    "W1 = getMatrix(P1);\n",
    "W2 = getMatrix(P2);\n",
    "W3 = getMatrix(P3);\n",
    "W4 = getMatrix(P4);\n",
    " \n",
    "W = (W1 + W2 + W3 + W4) / 4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:10.088000Z",
     "start_time": "2018-05-08T19:53:10.072004Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whichReference(P, ref_mat):\n",
    "    for i, p in enumerate(ref_mat):\n",
    "        if (getReference(P) == p).all():\n",
    "            sign = '-' if i > 3 else ''\n",
    "            num = str(i%4 + 1)\n",
    "            print(sign + 'P' + num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T19:53:10.891937Z",
     "start_time": "2018-05-08T19:53:10.859937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "P2\n",
      "-P4\n"
     ]
    }
   ],
   "source": [
    "# Calculo la salida para P3 \n",
    "print((getReference(P3) == P3).all())\n",
    " \n",
    "# Calculo la salida para -P3 \n",
    "print((getReference(-P3) == -P3).all())\n",
    " \n",
    "Ptot = [P1, P2, P3, P4, -P1, -P2, -P3, -P4]\n",
    "# Calculo la salida para P2 con error en el bit 4\n",
    "Pc = np.array([[1, 1, 1, 1, -1, -1, -1]])\n",
    "whichReference(Pc, Ptot)\n",
    "        \n",
    "# Calculo la salida para -P4 con un error en el bit 3\n",
    "Pc = np.array([[-1, -1, 1, 1, 1, -1, -1]])\n",
    "whichReference(Pc, Ptot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estabilidad de un patrón"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condición de estabilidad\n",
    "\n",
    "Normalmente, se espera que si se entrena con un patrón, el mismo debería ser igual a la salida, ya que se entrenó justamente la red para que eso pase. Dicho de otra forma, se debe cumplir:\n",
    "\n",
    "$ sign(h_i^\\nu) = 𝜉_i^\\nu $ para todo i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde $h_i^\\nu = \\sum_{j} w_{ij}^\\mu 𝜉_j^\\nu$ que es lo que resulta de entrenar la red de hopfield con un patrón ($𝜉$).\n",
    "\n",
    "Desarrollando los pesos $w$ se llega a que:\n",
    "\n",
    "$$ h_i^\\nu = \\frac{1}{N} \\sum_{j}\\sum_{\\mu} 𝜉_{i}^\\mu 𝜉_{j}^{\\mu} 𝜉_j^\\nu $$\n",
    "\n",
    "Sacamos finalmente factor común:\n",
    "\n",
    "\\begin{equation} \n",
    "h_i^\\nu = 𝜉_i^\\nu + \\frac{1}{N} \\sum_{j}\\sum_{\\mu \\neq \\nu} 𝜉_{i}^\\mu 𝜉_{j}^{\\mu} 𝜉_j^\\nu \n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la última ecuación se deduce que si el término de la derecha, es nulo, se cumple la condición de estabilidad.\n",
    "De hecho, gracias a la función signo que se debe aplicar sobre $h_i^\\nu$, se puede ser menos restrictivo con el término de la derecha y se puede decir que para que cumpla estabilidad debe darse la siguiente igualdad:\n",
    "\n",
    "\\begin{equation} \n",
    "    -𝜉_i^\\nu \\frac{1}{N} \\sum_{j}\\sum_{\\mu \\neq \\nu} 𝜉_{i}^\\mu 𝜉_{j}^{\\mu} 𝜉_j^\\nu < 1 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacidad de una red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiremos entonces la siguiente magnitud:\n",
    "\n",
    "$$ C_i^\\nu = 𝜉_i^\\nu \\frac{1}{N} \\sum_{j}\\sum_{\\mu \\neq \\nu} 𝜉_{i}^\\mu 𝜉_{j}^{\\mu} 𝜉_j^\\nu $$\n",
    "\n",
    "De la condición de estabilidad vista anteriormente.\n",
    "Si $ C_i^\\nu > 1 $ entonces tendremos un bit erroneo en la salida. Es decir, si se ingresa a la red con un patrón de entrenamiento, a la salida se obtendrá un patrón distinto al deseado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ C_i^\\nu $ depende únicamente de los patrones que queremos almacenar y de su cantidad. Si consideramos patrones de entrenamiento con igual ocurrencia de $  𝜉_{i}^\\mu = 1 $ y $  𝜉_{i}^\\mu = -1 $ entonces se puede calcular que la probablidad de que un bit sea inestable es:\n",
    "\n",
    "$$ P_{error} = Prob(C_i^\\nu > 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos 𝑃_𝑒𝑟𝑟𝑜𝑟:\n",
    "- N es la cantidad de bits de la entrada/salida.\n",
    "- p es la cantidad de patrones almacenados.\n",
    "\n",
    "_Nota:_ Suponemos que N y p >> 1 (es una condición aceptable y simplificará la matemática)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ 𝐶_𝑖^𝜈 $ es 1/N veces la suma de N*p (N*(p-1) para ser exactos) números aleatorios independientes que valen 1 o -1. Distribución de una suma de N*p valores que pueden valer 1 o -1 con p=0.5.\n",
    "\n",
    "Este problema puede ser modelado con una distribución binomial de media cero y varianza $𝜎^2=𝑝/𝑁$, pero por ser una suma de N términos, para $N*p>30$ puede ser modelado con una distribución gaussiana, valiéndonos del Teorema del Límite Central:\n",
    "\n",
    "![gauss](images/gaus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente tabla muestra las relaciones de p/N para obtener distintas probabilidades de error:\n",
    "![gauss](images/tablaerrores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede interpretar éstos resultados siendo que cuanto más grandes sean los patrones (N mayor) mayor será la matríz de Hopfield y mayor su capacidad. Por el contrario, cuantos más patrones se deseen agregar (p), más aumentará su probabilidad de error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efecto Avalancha\n",
    "\n",
    "- Este calculo solo se refiere a la probabilidad de error cuando ingresamos con patrones sin errores. Ingresar a la red con patrones cuyos bits tengan errores, hace que el error sea aún mayor.\n",
    "\n",
    "- En el peor caso puede ocurrir un efecto avalancha que haga que el número de errores sea tan grande que el patrón a la salida no tenga nada que ver con el patrón almacenado.\n",
    "\n",
    "- Se puede demostrar que este efecto avalancha ocurre cuando p>0.138N.\n",
    "\n",
    "- Otra definición de capacidad es que la mayoría de los patrones puedan ser recuperados perfectamente (los Nbits son recuperados correctamente con una probabilidad del 99%). Esto equivale a pedir una probabilidad de error p<0.01*N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estados Espúreos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de almacenar un patrón inverso, hay otros patrones que funcionan como atractores. Dicho atractor que no es ni el patrón mismo ni el patrón inverso se lo denomina como estádo espúreo.\n",
    "Un ejemplo de patrón espúreo, surge de hacer la cominación lineal de un número impar de patrones de entrenamiento, por ejemplo:\n",
    "\\begin{equation} \n",
    "𝜉_i^{mix} = sgn(\\pm𝜉_i^{\\mu_1}\\pm𝜉_i^{\\mu_2}\\pm𝜉_i^{\\mu_3})\n",
    "\\end{equation}\n",
    "\n",
    "Para un valor de $𝜉_i^{\\mu_1}$ dado $𝜉_i^{mix}$ tendrá el mismo signo 3 de las 4 combinaciones posibles de $𝜉_i^{\\mu_2}$ y $𝜉_i^{\\mu_3}$ por lo tanto $𝜉_i^{mix}$ estará a una distancia de Hamming N/4 de $𝜉_i^{\\mu_1}$. un razonamiento análogo nos lleva a que la distancia de Hamming de $𝜉_i^{mix}$ con respecto a $𝜉_i^{\\mu_2}$ y $𝜉_i^{\\mu_3}$ también es N/4.\n",
    "\n",
    "Esto implica que el producto escalar entre $𝜉_i^{mix}$ y $𝜉_i^{\\mu_1}$ valga N/2.\n",
    "\n",
    "Por lo tanto, nos queda:\n",
    "\\begin{equation} \n",
    "h_i^{mix} =  \\frac{1}{N} \\sum_{j\\mu} 𝜉_{i}^\\mu 𝜉_{j}^{\\mu} 𝜉_j^{mix} = \\frac{1}{2}𝜉_i^{\\mu_2}+\\frac{1}{2}𝜉_i^{\\mu_3}+crossterms\n",
    "\\end{equation}\n",
    "\n",
    "Los cross-terms tienen la particularidad estadística (distribuación binamial) analizada anteriormente, por lo que la ecuación anterior cumple con el criterio de estabilidad. Por lo tanto $𝜉_i^{mix}$ es un patrón almacenado en la red.\n",
    "\n",
    "Además Amit (1985) demostró que existen otros estados espúreos en la red que no guardan correlación con los estados ya almacenados.\n",
    "\n",
    "Por lo tanto, la red de Hopfield está limitada no solo por la probabilidad del error de bit, sino que admás por la existencia de estados espúreos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mecánica estadística de sistemas magnéticos\n",
    "\n",
    " - Hay una relación estrecha entre las redese de Hopfield y algunos modelos magnéticos sencillos de la física estadística.\n",
    " - La analogía es muy útil cuando se utilizan unidades estocásticas, de donde se establece el concepto de temperatura de la red.\n",
    " - Repasaremos algunos conceptos básicos de materiales magnéticos.\n",
    "   - Se pueden representar como un conjunto de dipolos magnéticos de tamaño atómico, dispuestos de acuerdo a una estructura cristalina. A cada dipolo lo llamaremos SPIN.\n",
    "   - Cada spin podrá estar alineado en dos direcciones posibles.\n",
    "   - cada spin será identificado por una variable $S_i$. Si el spin i está orientado hacia arriba, entonces $S_i=1$. Si el spin i está orientado hacia abajo $S_i=-1$\n",
    "   \n",
    "   \n",
    " - La representación gráfica de un material magnético nos queda de la siguiente forma:\n",
    " ![field](images/field.png)\n",
    " - Para la red neuronal análoga, una neurona activada equivale a un spin +1 y una neurona desactivada un spin -1.\n",
    " - El comportamiento de cada spin queda definido por el campo magnético existente en su ubicación, el cual está compuesto por el campo generado por los otros spins (campo interno), mas la contribución de un posible campo magnético externo.\n",
    " - Dichas contribuciones sobre el spin i pueden ser expresadas como:\n",
    "\\begin{equation} \n",
    "h_i =   \\sum_{j} w_{ij} S_{j}+ h^{ext}\n",
    "\\end{equation}\n",
    " - Donde cada coeficiente $w_{ij}$ miden la influencia de cada spin j sobre el spin i. En un material magnético estos coeficientes son simétricos.\n",
    " - El campo magnético $h_i$ a bajas temperaturas define la dinámica del spin i ya que el spin tiende a alinearse según la dirección de $h_i$.\n",
    "   - Por lo tanto el valor de $S_i = sgn(h_i)$\n",
    "   - Esta actualización se hace en forma asincrónica.\n",
    " - Otra forma de definir la interacción de los spins es a través de la energía potencial vinculada a esa interacción. Para un material magnético cuyos spins están influenciados según $\\omega_{ij}$, su energía potencial es:\n",
    "\\begin{equation} \n",
    "H =  -\\frac{1}{2} \\sum_{ij} w_{ij} S_{i}S_{j}- h^{ext}\\sum_{i}S_i\n",
    "\\end{equation}\n",
    " - Por lo tanto la analogía con una red de Hopfield es completa\n",
    "   - Los coeficientes de interacción entre los spins son los pesos\n",
    "   - La entrada a una neurona corresponde al estado de cada uno de los spins.\n",
    "   - La suma resultante de la interacción de todos los spins j en un spin i mas el campo externo, corresponde a la salida del combinador lineal.\n",
    " - A este modelo de un material ferromagnético se lo denomina ** Modelo de Issing **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efecto de la temperatura en el modelo de Issing\n",
    " - El modelo anterior aplica solamente a temperaturas muy bajas, cercanas al 0 K.\n",
    " - Para temeraturas más altas, el comportamiento de los spins se vuelve estocástico y su estado de alineación esta determinado por: $S_i = +1$ on probabilidad $g(h_i)$ y $S_i$ con probabilidad $1-g(h_i)$.\n",
    " - La función $g(h_i)$ depende de la temperatura del sistema:\n",
    "\\begin{equation} \n",
    "g(h) = f_\\beta(h)\\equiv \\frac{1}{1+exp(-2\\beta h)}\n",
    "\\end{equation}\n",
    " - Ya que $\\beta=\\frac{1}{\\kappa_B T}$, donde $\\kappa_B$ es la constante de Boltzmann y vale $1.38*10^{16\\frac{erg}{K}}$.\n",
    " - Dado que: $1-f_\\beta(h)=f_\\beta(-h)$, podemos escribir: $Prob(S_i = \\pm1)=f_\\beta(\\pm h_i)=\\frac{1}{1+exp(\\mp2\\beta h_i)}$\n",
    " - La temperatura regula qué tan abrupta es la variación de la pdf alrededor de h=0.\n",
    " - A medida que $T\\rightarrow0$, vemos como el modelo estocástico se reduce al modelo determinístico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado de equilibrio de un spin\n",
    " - Vamos a analizar cual es la magnetización promedio de un material ferromagnético.\n",
    " - Para empezar, supongamos que tenemos un material ferromagnético de un solo spin.\n",
    "   - Esto implicará que el único campo magnético será el externo.\n",
    "   - Su valor promedio tendrá la forma:\n",
    "\\begin{equation}\n",
    "\t\\langle S \\rangle = Prob(+1).(+1)+ Prob(-1).(-1) \\\\\n",
    "    = \\frac{1}{1+e^{-2\\beta h}}-\\frac{1}{1+e^{2\\beta h}}=\\frac{e^{\\beta h}}{e^{\\beta h}+e^{-\\beta h}}-\\frac{e^{-\\beta h}}{e^{-\\beta h}+e^{\\beta h}} \\\\\n",
    "    =tangh(\\beta h)\n",
    "\\end{equation}\n",
    " - La tanh tiene la misma forma que la distribución de probabilidades, pero varia entre -1 y 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teroría del campo medio\n",
    " - Ahora queremos analizar que pasa en un material ferromagnético con más de un spin.\n",
    " - La evolución del estado de un spin i depende de:\n",
    "\\begin{equation}\n",
    " h_i=\\sum_i w_{ij}S_j+h^{ext}\n",
    "\\end{equation}\n",
    "   - Lo cual involucra variables S_j que fluctuán en el tiempo.\n",
    "  - Como el análisis de un material ferromagnético es muy complejo, se utiliza la teoría del campo medio para analizar el comportamiento de material y su magnetización promedio.\n",
    "  - Más allá del interés que pueda tener físicamente este análisis, más adelante noss brindará una herramienta más para analizar redes neuoronales.\n",
    "  - La idea consiste en reemplazar a cada uno de los $h_i$ resultantes en  cada spin, por su valor medio:\n",
    "  \\begin{equation}\n",
    "\t\\langle h_i \\rangle =\\sum_j w_{ij}\\langle S_j \\rangle + h^{ext}\n",
    "   \\end{equation}\n",
    "  - El mismo depende de los valores promedios de los spins del material.\n",
    "  - A partir de lo visto en \"Esta de equilibrio de un spin\" podemos calcular los valores de campo magnético de cada spin del material como:\n",
    "  \\begin{equation}\n",
    "\t\\langle S_i \\rangle = tanh(\\beta \\langle h_i \\rangle) = tanh(\\beta\\sum_j w_{ij}\\langle S_j \\rangle +\\beta h^{ext})\n",
    "  \\end{equation}\n",
    "    - Lo cual nos da N ecuaciones no lineales con N incógnitas, pero sin que en ningún lado aparezcan variables aleatorias.\n",
    "  - La idea es reemplazar todos los sins salvo por sus valores medios, de esta forma podemos analizar como evoluciona un spin en particular.\n",
    "  - A medida que la cantidad de spins aumenta, este modelo se vuelve cada vez más preciso.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Material ferromagnético\n",
    " - Todos los $w_{ij}$ son positivos. Esto hace que todos los spins tiendan a alinearse los unos a los otros.\n",
    " - A partir de cierta temperatura $T_C$, sin campo externo,$\\langle S \\rangle =0$. Por debajo de esa temperatura $T_C,\\langle S \\rangle \\neq 0$\n",
    "   - Por ejemplo el hierro pierde sus propiedades magnéticas a 770 grados.\n",
    " - El modelo más sencillo de material ferromagnético está dado por $w_{ij}=\\frac{J}{N}$\n",
    "   - Para un ferromagnético de N spins.\n",
    "   - Este modelo tiene su análogo en la red de Hopfield. Si J=1, corresponde a un solo patrón almacenado, cuyos bits valen todos 1.\n",
    "   - Es decir. hay dos atractores, uno con todos los valores en 1 y otro con todos los valores en -1. Dicho de otra forma, todos los spins alineados hacia arriaba y todos los spins alineados hacia abajo.\n",
    "   - Si hablamos de una red con varios patrones almacenados, el fenómeno es como si habláramos de un material ferromagnético cuyos valores de spins estables ya no son todos 1 o todos -1, sino que aparecen otras opciones.\n",
    " - En temperatura cero si la mayoría de los spins apuntan en una dirección, podemos dejar que el sistema evolucione en el tiempo hasta que todos queden apuntando en esa dirección.\n",
    " - Para una temperatura dada, podemos analizar que pasa con el material ferromagnético usando la teoría de campo medio. En el caso ferromagnético con $w_{ij}=\\frac{J}{N}$ nos queda:\n",
    "   \\begin{equation}\n",
    "\t\\langle S \\rangle = tanh(\\beta J\\langle S \\rangle) \n",
    "  \\end{equation}\n",
    "    - Para todos los spins sin la presencia de campo magnético externo.\n",
    "    - Esta ecuación puede ser resuelta gráficamente:\n",
    "    ![ferrofun](images/ferrofun.png)\n",
    " - La solució varia según $\\beta J$ sea mayor o menor a 1.\n",
    " - Esto hace que haya una temperatura crítica a partir de la cual la media del valor spin se hace cero \n",
    " \\begin{equation}\n",
    "\tT_C=\\frac{J}{\\kappa_B}\n",
    "  \\end{equation}\n",
    "  - El gráfico del valor medio del spin en función de la temperatura (para una de las soluciones) quedaría:\n",
    "  ![temp](images/temp.png)\n",
    "  - Los spins pueden encontrarse predominantemente hacia arriba o hacia abajo cuando $T<T_C$.\n",
    "  - Si $N\\rightarrow\\infty$, entonces el sistema va a permanecer en el estadoe n el que se encuentre.\n",
    "  - Siendo que en general N es lo suficientemente grande, la teoría del campo medio describe correctamente el comportamiento de un material ferromagnético en función de la temperatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes estocásticas\n",
    " - Haremos que el comportamiento de las neuronas de una red de Hopfield sea idéntico al de los spins en el modelo de Issing.\n",
    " - Es decir, reemplazaremos la función de activación por una función de activación estocástica que cumpla con:\n",
    " \\begin{equation}\n",
    " Prob(S_i = \\pm1)=f_\\beta(\\pm h_i)=\\frac{1}{1+exp(\\mp2\\beta h_i)}\n",
    " \\end{equation}\n",
    "   - Para la neurona i seleccionada en forma aleatoria para ser actualizara.\n",
    " - Se puede definir uns pseudo-temperatura de la red como: $\\beta \\equiv \\frac{1}{T}$\n",
    " - Dicha temperatura modifica la pendiente de la sigmoidea alrededor de h=0.\n",
    "   - A baja temperatura la sigmoidea se transforma en una función escalón y nos queda una red determinística.\n",
    "   - A medida que sube la temperatura la pendiente es cada vez menos pronunciada.\n",
    " - El uso de unidades estocásticas no solos nos permite modelar el comportamiento de una red neuronal real, sino que es útil en muchas situaciones ya que veremsos que nos permite eliminar varios mínimos locales de la función de enrgía pertenecientes a estados espurios.\n",
    "   - En general los mínimos correspondientes a estados espurios tienen mayor energía (menor estabilidad) que los patrones almacenados.\n",
    " - En base a esto, debemos analizar los los estaos de equilibrio de una red nauronal, esto es, aplicar el concepto de teoría de campo medio para ver qué pasa con cada una de las neuronas a medida que el sistema evoluciona.\n",
    "   - Se puede probar que una red de pesos simétricos con función de energía asociada, tiene de a converger a un estado de equilibrio.\n",
    " - Si bien no podemos hablar de configuraciones estables en función de valores determinísiticos de $S_i$, si podemos ver en que valores medidos de $S_i$ se estabiliza cada una de las neuronas.\n",
    " ### Redes estocásticas (Teoría de campo medio)\n",
    " - Nos mantendremos en el casop de $p<<N$ o bien $N\\rightarrow \\infty$.\n",
    " - Podemos escribir las ecuaciones de campo medio como:\n",
    "   \\begin{equation}\n",
    "\t\\langle S_i \\rangle = tanh(\\frac{\\beta}{N}\\sum_{i,\\mu}𝜉_{i}^{\\mu}𝜉_{j}^{\\mu} \\langle S_i \\rangle)\n",
    "  \\end{equation}\n",
    "   - La solución es muy difícil de obtener ya que tenemos un sistema de N ecuaciones no lineales con N incógnitas.\n",
    " - Siguiendo el ejemplo del material ferromagnético podemos suponer que:\n",
    " \\begin{equation}\n",
    "\t\\langle S_i \\rangle =m𝜉_{i}^{\\nu}\n",
    "  \\end{equation}\n",
    "   - Es una solución del sistema de ecuaciones.\n",
    "   - Es decir, una de las soluciones es proporcional a uno de los patrones almacenados.\n",
    " - Ya vimos que en el caso de la red determinística esto es cierto y con m=1.\n",
    " - Siguiendo la suposición, podemos escribir:\n",
    "    \\begin{equation}\n",
    "\tm𝜉_{i}^{\\nu} = tanh(\\frac{\\beta}{N}\\sum_{i,\\mu}𝜉_{i}^{\\mu}𝜉_{j}^{\\mu}m𝜉_j^{\\nu})\n",
    "  \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como en el de la red estocástica podemos escribir la sumatoria como un término proporcional a $𝜉_{i}^{\\nu}$ mas un crosstalk term tiende a cero y nos queda:\n",
    "   \\begin{equation}\n",
    "\tm𝜉_{i}^{\\nu} = tanh(\\beta m 𝜉_j^{\\nu})\n",
    "  \\end{equation}\n",
    "- Pero como tanh es una función impar, podemos escribir independientemente del valor de $𝜉_j^{\\nu}$ : \n",
    "   \\begin{equation}\n",
    "\tm = tanh(\\beta m )\n",
    "  \\end{equation}\n",
    "- Esto es muy similar a lo que encontramos en el caso de la magnetización de un ferromagnético, por lo que podemos resolver de la misma manera (en forma gráfica); por lo tanto la temperatura crítica es igual a $T_C$ es igual a 1 para una red estocástica con $p<<N$.\n",
    "- Adaptando el problema de Issing a una red neuronal, podemos escribir:\n",
    "   \\begin{equation}\n",
    "   m = \\frac{\\langle S_i \\rangle}{𝜉_j^{\\nu}}=\\text{Prob(bit is correct)} - \\text{Prob(bit is incorrect)}\n",
    "  \\end{equation}\n",
    "- Y por lo tanto el número promedio de bits correctos es:\n",
    "   \\begin{equation}\n",
    "   \\langle N_correct \\rangle = \\frac{1}{2}N(1+m)\n",
    "  \\end{equation}\n",
    "- Lo podemos graficar en función de de la temperatura:\n",
    "![temp2](images/temp2.png)\n",
    "- Si bien se eliminan estados espurios de mayor energía, otros estados como los inversos permanecen en la red.\n",
    "- Los estados mixtos siguen presente, pero cada uno con una temperatura crítica distinta a partir de la cual deja de ser estable.\n",
    "- La temperatura más alta a partir de la cual los estados mixtos dejan de ser estables es la correspondiente a 3 estados mezclados. Dicha temperatura vale T=0.46.\n",
    "- Esto muestra como el ruido (a través del concepto de temperatura) mejora la performance de la red.\n",
    "- La siguiente figura muestra esquemáticamente cómo varía la función de energía al aumentar la temperatura de la red:\n",
    "![espures](images/espureos.png)\n",
    "- Nótese que el análisis de teoría de campo medio es independiente del modo de actualización (ya sea sincrónica o asincrónica) ya que se independeiza de la dinámica temporal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Weak Dilution\n",
    "\n",
    "Vamos a simular el proceso de muerte celular removiendo algunos pesos de la red aleatoriamente. Es decir:\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "f(x) = \\left\\lbrace\n",
    "\\begin{array}{ll}\n",
    "Hebb Value & c\\\\\n",
    "0 & 1 - c\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Donde c es una probabilidad que representa la proporción de los pesos que mantendremos.\n",
    "\n",
    "Definamos $𝐶_{𝑖𝑗}=1$ si deseamos mantener el peso ij y $𝐶_{𝑖𝑗}=0$ en caso contrario. Podemos definir:\n",
    "\n",
    "$$ w_{ij} = C_{ij} w_{ij}^{Hebb} $$\n",
    "\n",
    "Quedando el valor a la entrada de la función de activación:\n",
    "\n",
    "$$ h_{ij} = \\sum_{j} C_{ij} w_{ij}^{Hebb} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como solo es removida una fracción de las conexiones, a medida que N∞, la cantidad de conexiones existentes seguirá siendo infinita. Por lo tanto podemos escribir:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\langle h_i \\rangle = c \\sum_{j} w_{ij}^{Hebb} \\langle S_i \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "Esto hace que varíen los valores medios a la entrada de los elementos estocásticos haciendo que la temperatura efectiva de la red sea la temperatura original multiplicada por 1/c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strong Dilution\n",
    "\n",
    "Analicemos qué pasa cuando un número infinitesimal de conexiones permanecen correctamente.\n",
    "\n",
    "Llamaremos K al número promedio de conexiones que permanecen desde y hasta cada una de las unidades.\n",
    "$K < log N$ cuando $N \\to ∞$.\n",
    "\n",
    "La dilución se hace independientemente para $𝑤_{𝑖𝑗}$ y $𝑤_{𝑗𝑖}$, por lo que $𝐶_{𝑖𝑗}$ y $𝐶_{𝑗𝑖}$ son variables aleatorias independientes. La matrix W ya no es simétrica.\n",
    "\n",
    "Definiremos los pesos como:\n",
    "\n",
    "\\begin{equation}\n",
    "\tw_{ij} = \\frac{𝐶_{𝑖𝑗}}{K} \\sum_{\\mu} 𝜉_{i}^{\\mu}𝜉_{j}^{\\mu}\n",
    "\\end{equation}\n",
    "\n",
    "El 1/N fue reemplazado por 1/K para tener todos los valores cercanos a la unidad.\n",
    "Podemos expresar $ℎ_𝑖$ como:\n",
    "\n",
    "\\begin{equation}\n",
    "\th_{i} = \\sum_{j} w_{ij} S_j = \\frac{1}{K} \\sum_{j} 𝐶_{𝑖𝑗}  \\sum_{\\mu} 𝜉_{i}^{\\mu}𝜉_{j}^{\\mu} S_j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un patrón almacenado 𝜉_𝑖^𝜈 podemos desglosar ℎ_𝑖 en función de la salida esperada y el término de crosstalk:\n",
    "\n",
    "\\begin{equation}\n",
    "\th_{i} = \\sum_{j} w_{ij} S_j = \\frac{1}{K} 𝜉_{i}^{\\nu} \\sum_{j} 𝐶_{𝑖𝑗} 𝜉_{j}^{\\nu} S_j + \\eta_i^\\nu\n",
    "\\end{equation}\n",
    "\n",
    "Ddonde\n",
    "$$ \\eta_i^\\nu = \\frac{1}{K} \\sum_{\\mu \\neq \\nu } 𝜉_{i}^{\\mu} \\sum_{j} 𝐶_{𝑖𝑗} 𝜉_{j}^{\\nu} S_j $$\n",
    "\n",
    "En este caso el crosstalk term depende del estado actual $𝑆_𝑗$.\n",
    "Si hacemos $𝑆_𝑖=𝜉_𝑖^𝜈$, entonces el término de la derecha da en promedio $ 𝜉_𝑖^𝜈$ ya que $⟨∑_𝑗 𝐶_{𝑖𝑗} ⟩=𝐾$. El término de crosstalk se reduce a 1/𝐾 veces la suma de aproximadamente K*p variables aleatorias independientes (cuyo valor varía entre 1 y -1).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como $𝑆_𝑖 = 𝑠𝑔𝑛(ℎ_𝑖)$ podemos escribir:\n",
    "\n",
    "$$ m_\\nu = \\frac{1}{N} \\sum_i  sign( 𝜉_{i}^{\\nu} h_i) = \\frac{1}{N} \\sum_i  sign(m_\\nu + 𝜉_{i}^{\\nu} \\eta_i^\\nu) $$\n",
    "\n",
    "Para pasar del segundo término al tercero, utilizamos:\n",
    "\n",
    "$$ h_i = \\sum_j w_{ij} S_j = \\frac{1}{K} 𝜉_{i}^{\\nu} \\sum_{j} 𝐶_{𝑖𝑗} 𝜉_{j}^{\\nu} S_j + \\eta_i^\\nu $$\n",
    "\n",
    "Para saber cuánto se parece el estado actual a un patrón estable podemos ver cuánto vale la media de $𝑚_𝜈$. La misma nos queda:\n",
    "\n",
    "$$ h_i = \\int{dn \\cdot P(\\eta) \\cdot sign(m_\\nu + \\eta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede demostrar que si $K<<M$ entonces queda:\n",
    "\n",
    "$$ m_\\nu = erf(\\frac{m_\\nu}{\\sqrt{2\\alpha '}}) $$\n",
    "\n",
    "En esencia es un problema muy similar al de x=tanh(x), ya que la función tanh y erf son muy similares:\n",
    "\n",
    "![strong](images/strongdilution.png)\n",
    "\n",
    "La pendiente en el origen en vez de valer 1 vale $\\frac{2}{\\sqrt{𝜋}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede resolver gráficamente y obtenemos una solución de la misma forma que $<S>$ en función de la temperatura, pero en este caso $𝛼_𝑐^′$ actúa de temperatura. \n",
    "\n",
    "Hay un valor crítico de $𝛼_𝑐^′=2/𝜋$ a partir del cual la única solución es $𝑚_𝑣=0$ , lo cual implica que se perdió la capacidad de la red de almacenar patrones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "118px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
